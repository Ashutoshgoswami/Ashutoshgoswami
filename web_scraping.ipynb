{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMzYO8gMvxf09dqtcrwuM9E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashutoshgoswami/Ashutoshgoswami/blob/main/web_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  !pip install requests beautifulsoup4 pandas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPouh7W0M1_p",
        "outputId": "0f801495-6c73-4493-fbff-554c6fb4fb65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# URL of the weather page on Timeanddate.com\n",
        "url = \"https://www.timeanddate.com/weather/india/bhopal/ext\"\n",
        "\n",
        "# Send a request to the website\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Parse the HTML content\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Locate the table with id 'wt-ext'\n",
        "    table = soup.find('table', {'id': 'wt-ext'})\n",
        "\n",
        "    # Extract the table headers\n",
        "    headers = [header.text.strip() for header in table.find_all('th')]\n",
        "\n",
        "    # Extract the table rows\n",
        "    rows = []\n",
        "    for row in table.find('tbody').find_all('tr'):\n",
        "        cells = [cell.text.strip() for cell in row.find_all('td')]\n",
        "\n",
        "        # If the row has fewer columns, fill the missing columns with None\n",
        "        if len(cells) < len(headers):\n",
        "            cells += [None] * (len(headers) - len(cells))\n",
        "        rows.append(cells)\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame(rows, columns=headers)\n",
        "\n",
        "    # Print the DataFrame\n",
        "    print(df)\n",
        "else:\n",
        "    print(\"Failed to retrieve the webpage\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4530e_ZM48A",
        "outputId": "28530dfa-1f06-4453-cbef-d48ee577df6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Conditions                          Comfort Precipitation     Sun Day  \\\n",
            "0     108 / 86 °F                   Partly cloudy.        106 °F   6 mph   ↑   \n",
            "1     110 / 88 °F           Increasing cloudiness.        109 °F  10 mph   ↑   \n",
            "2     113 / 87 °F           Increasing cloudiness.        108 °F  16 mph   ↑   \n",
            "3     112 / 89 °F                          Cloudy.        108 °F  15 mph   ↑   \n",
            "4     108 / 86 °F                Afternoon clouds.        106 °F  18 mph   ↑   \n",
            "5     105 / 79 °F                           Sunny.        103 °F  15 mph   ↑   \n",
            "6     104 / 80 °F                Afternoon clouds.        103 °F   9 mph   ↑   \n",
            "7     103 / 82 °F                   Mostly cloudy.        101 °F   8 mph   ↑   \n",
            "8     104 / 84 °F           Increasing cloudiness.        103 °F  11 mph   ↑   \n",
            "9     104 / 86 °F     Showers late. Mostly cloudy.        103 °F  13 mph   ↑   \n",
            "10    104 / 86 °F  Showers late. Afternoon clouds.        104 °F  12 mph   ↑   \n",
            "11    105 / 86 °F                          Cloudy.        107 °F  14 mph   ↑   \n",
            "12    106 / 87 °F                          Cloudy.        109 °F  15 mph   ↑   \n",
            "13    105 / 87 °F                   Broken clouds.        106 °F  16 mph   ↑   \n",
            "14    104 / 85 °F                Scattered clouds.        100 °F  17 mph   ↑   \n",
            "\n",
            "        Temperature Weather      Feels Like  ... WedMay 29 ThuMay 30  \\\n",
            "0   17%          0%       -        7 (High)  ...      None      None   \n",
            "1   18%          0%       -        7 (High)  ...      None      None   \n",
            "2   12%          0%       -        7 (High)  ...      None      None   \n",
            "3   14%          0%       -        7 (High)  ...      None      None   \n",
            "4   18%          0%       -        7 (High)  ...      None      None   \n",
            "5   21%          0%       -  10 (Very high)  ...      None      None   \n",
            "6   23%          0%       -  10 (Very high)  ...      None      None   \n",
            "7   21%          4%       -        7 (High)  ...      None      None   \n",
            "8   22%          5%       -        7 (High)  ...      None      None   \n",
            "9   22%         47%   0.02\"        7 (High)  ...      None      None   \n",
            "10  23%         47%   0.03\"        7 (High)  ...      None      None   \n",
            "11  26%         52%   0.02\"        7 (High)  ...      None      None   \n",
            "12  25%          6%       -    3 (Moderate)  ...      None      None   \n",
            "13  22%          6%       -    3 (Moderate)  ...      None      None   \n",
            "14  14%          5%       -        7 (High)  ...      None      None   \n",
            "\n",
            "   FriMay 31 SatJun 1 SunJun 2 MonJun 3 TueJun 4 WedJun 5 ThuJun 6 FriJun 7  \n",
            "0       None     None     None     None     None     None     None     None  \n",
            "1       None     None     None     None     None     None     None     None  \n",
            "2       None     None     None     None     None     None     None     None  \n",
            "3       None     None     None     None     None     None     None     None  \n",
            "4       None     None     None     None     None     None     None     None  \n",
            "5       None     None     None     None     None     None     None     None  \n",
            "6       None     None     None     None     None     None     None     None  \n",
            "7       None     None     None     None     None     None     None     None  \n",
            "8       None     None     None     None     None     None     None     None  \n",
            "9       None     None     None     None     None     None     None     None  \n",
            "10      None     None     None     None     None     None     None     None  \n",
            "11      None     None     None     None     None     None     None     None  \n",
            "12      None     None     None     None     None     None     None     None  \n",
            "13      None     None     None     None     None     None     None     None  \n",
            "14      None     None     None     None     None     None     None     None  \n",
            "\n",
            "[15 rows x 33 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL of the page to be scraped\n",
        "url = \"https://www.timeanddate.com/weather/india/bhopal/ext\"\n",
        "\n",
        "# Send a GET request to the website\n",
        "response = requests.get(url)\n",
        "response.raise_for_status()  # Check if the request was successful\n",
        "\n",
        "# Parse the HTML content\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Locate the main table by ID\n",
        "table = soup.find('table', id='wt-ext')\n",
        "\n",
        "# Extract the table headers\n",
        "headers = table.find('thead').find_all('th')\n",
        "header_titles = [header.text.strip() for header in headers]\n",
        "\n",
        "# Extract the table rows\n",
        "rows = table.find('tbody').find_all('tr')\n",
        "\n",
        "# Extract data from each row\n",
        "weather_data = []\n",
        "for row in rows:\n",
        "    columns = row.find_all('td')\n",
        "    column_data = [column.text.strip() for column in columns]\n",
        "    weather_data.append(column_data)\n",
        "\n",
        "# Display the extracted data\n",
        "print(\"Table Headers:\", header_titles)\n",
        "for data in weather_data:\n",
        "    print(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrQxw8eQUrl7",
        "outputId": "eb55c8cf-eadd-4cfe-c533-5666f33840ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table Headers: ['', 'Conditions', 'Comfort', 'Precipitation', 'Sun', 'Day', '', 'Temperature', 'Weather', 'Feels Like', 'Wind', '', 'Humidity', 'Chance', 'Amount', 'UV', 'Sunrise', 'Sunset']\n",
            "['', '108 / 86\\xa0°F', 'Partly cloudy.', '106\\xa0°F', '6 mph', '↑', '17%', '0%', '-', '7 (High)', '5:35 am', '6:59 pm']\n",
            "['', '110 / 88\\xa0°F', 'Increasing cloudiness.', '109\\xa0°F', '10 mph', '↑', '18%', '0%', '-', '7 (High)', '5:35 am', '6:59 pm']\n",
            "['', '113 / 87\\xa0°F', 'Increasing cloudiness.', '108\\xa0°F', '16 mph', '↑', '12%', '0%', '-', '7 (High)', '5:35 am', '6:59 pm']\n",
            "['', '112 / 89\\xa0°F', 'Cloudy.', '108\\xa0°F', '15 mph', '↑', '14%', '0%', '-', '7 (High)', '5:34 am', '7:00 pm']\n",
            "['', '108 / 86\\xa0°F', 'Afternoon clouds.', '106\\xa0°F', '18 mph', '↑', '18%', '0%', '-', '7 (High)', '5:34 am', '7:00 pm']\n",
            "['', '105 / 79\\xa0°F', 'Sunny.', '103\\xa0°F', '15 mph', '↑', '21%', '0%', '-', '10 (Very high)', '5:34 am', '7:01 pm']\n",
            "['', '104 / 80\\xa0°F', 'Afternoon clouds.', '103\\xa0°F', '9 mph', '↑', '23%', '0%', '-', '10 (Very high)', '5:34 am', '7:01 pm']\n",
            "['', '103 / 82\\xa0°F', 'Mostly cloudy.', '101\\xa0°F', '8 mph', '↑', '21%', '4%', '-', '7 (High)', '5:34 am', '7:02 pm']\n",
            "['', '104 / 84\\xa0°F', 'Increasing cloudiness.', '103\\xa0°F', '11 mph', '↑', '22%', '5%', '-', '7 (High)', '5:34 am', '7:02 pm']\n",
            "['', '104 / 86\\xa0°F', 'Showers late. Mostly cloudy.', '103\\xa0°F', '13 mph', '↑', '22%', '47%', '0.02\"', '7 (High)', '5:34 am', '7:03 pm']\n",
            "['', '104 / 86\\xa0°F', 'Showers late. Afternoon clouds.', '104\\xa0°F', '12 mph', '↑', '23%', '47%', '0.03\"', '7 (High)', '5:33 am', '7:03 pm']\n",
            "['', '105 / 86\\xa0°F', 'Cloudy.', '107\\xa0°F', '14 mph', '↑', '26%', '52%', '0.02\"', '7 (High)', '5:33 am', '7:03 pm']\n",
            "['', '106 / 87\\xa0°F', 'Cloudy.', '109\\xa0°F', '15 mph', '↑', '25%', '6%', '-', '3 (Moderate)', '5:33 am', '7:04 pm']\n",
            "['', '105 / 87\\xa0°F', 'Broken clouds.', '106\\xa0°F', '16 mph', '↑', '22%', '6%', '-', '3 (Moderate)', '5:33 am', '7:04 pm']\n",
            "['', '104 / 85\\xa0°F', 'Scattered clouds.', '100\\xa0°F', '17 mph', '↑', '14%', '5%', '-', '7 (High)', '5:33 am', '7:05 pm']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# URL of the page to be scraped\n",
        "url = \"https://www.timeanddate.com/weather/india/bhopal/ext\"\n",
        "\n",
        "# Send a GET request to the website\n",
        "response = requests.get(url)\n",
        "response.raise_for_status()  # Check if the request was successful\n",
        "\n",
        "# Parse the HTML content\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Locate the main table by ID\n",
        "table = soup.find('table', id='wt-ext')\n",
        "\n",
        "# Extract the table headers\n",
        "headers = table.find('thead').find_all('th')\n",
        "header_titles = [header.text.strip() for header in headers]\n",
        "\n",
        "# Extract the table rows\n",
        "rows = table.find('tbody').find_all('tr')\n",
        "\n",
        "# Extract data from each row\n",
        "weather_data = []\n",
        "for row in rows:\n",
        "    columns = row.find_all('td')\n",
        "    column_data = [column.text.strip() for column in columns]\n",
        "    # Pad the row with None if it's shorter than the header_titles\n",
        "    if len(column_data) < len(header_titles):\n",
        "        column_data.extend([None] * (len(header_titles) - len(column_data)))\n",
        "    weather_data.append(column_data)\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(weather_data, columns=header_titles[:len(weather_data[0])])\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)\n",
        "\n",
        "# Optionally, save the DataFrame to a CSV file\n",
        "df.to_csv('weather_data.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMrGjRcFb6YW",
        "outputId": "3fd2e6e1-50a3-4b72-84df-283605f30678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Conditions                          Comfort Precipitation     Sun Day  \\\n",
            "0     108 / 86 °F                   Partly cloudy.        106 °F   6 mph   ↑   \n",
            "1     110 / 88 °F           Increasing cloudiness.        109 °F  10 mph   ↑   \n",
            "2     113 / 87 °F           Increasing cloudiness.        108 °F  16 mph   ↑   \n",
            "3     112 / 89 °F                          Cloudy.        108 °F  15 mph   ↑   \n",
            "4     108 / 86 °F                Afternoon clouds.        106 °F  18 mph   ↑   \n",
            "5     105 / 79 °F                           Sunny.        103 °F  15 mph   ↑   \n",
            "6     104 / 80 °F                Afternoon clouds.        103 °F   9 mph   ↑   \n",
            "7     103 / 82 °F                   Mostly cloudy.        101 °F   8 mph   ↑   \n",
            "8     104 / 84 °F           Increasing cloudiness.        103 °F  11 mph   ↑   \n",
            "9     104 / 86 °F     Showers late. Mostly cloudy.        103 °F  13 mph   ↑   \n",
            "10    104 / 86 °F  Showers late. Afternoon clouds.        104 °F  12 mph   ↑   \n",
            "11    105 / 86 °F                          Cloudy.        107 °F  14 mph   ↑   \n",
            "12    106 / 87 °F                          Cloudy.        109 °F  15 mph   ↑   \n",
            "13    105 / 87 °F                   Broken clouds.        106 °F  16 mph   ↑   \n",
            "14    104 / 85 °F                Scattered clouds.        100 °F  17 mph   ↑   \n",
            "\n",
            "        Temperature Weather      Feels Like     Wind          Humidity Chance  \\\n",
            "0   17%          0%       -        7 (High)  5:35 am  6:59 pm     None   None   \n",
            "1   18%          0%       -        7 (High)  5:35 am  6:59 pm     None   None   \n",
            "2   12%          0%       -        7 (High)  5:35 am  6:59 pm     None   None   \n",
            "3   14%          0%       -        7 (High)  5:34 am  7:00 pm     None   None   \n",
            "4   18%          0%       -        7 (High)  5:34 am  7:00 pm     None   None   \n",
            "5   21%          0%       -  10 (Very high)  5:34 am  7:01 pm     None   None   \n",
            "6   23%          0%       -  10 (Very high)  5:34 am  7:01 pm     None   None   \n",
            "7   21%          4%       -        7 (High)  5:34 am  7:02 pm     None   None   \n",
            "8   22%          5%       -        7 (High)  5:34 am  7:02 pm     None   None   \n",
            "9   22%         47%   0.02\"        7 (High)  5:34 am  7:03 pm     None   None   \n",
            "10  23%         47%   0.03\"        7 (High)  5:33 am  7:03 pm     None   None   \n",
            "11  26%         52%   0.02\"        7 (High)  5:33 am  7:03 pm     None   None   \n",
            "12  25%          6%       -    3 (Moderate)  5:33 am  7:04 pm     None   None   \n",
            "13  22%          6%       -    3 (Moderate)  5:33 am  7:04 pm     None   None   \n",
            "14  14%          5%       -        7 (High)  5:33 am  7:05 pm     None   None   \n",
            "\n",
            "   Amount    UV Sunrise Sunset  \n",
            "0    None  None    None   None  \n",
            "1    None  None    None   None  \n",
            "2    None  None    None   None  \n",
            "3    None  None    None   None  \n",
            "4    None  None    None   None  \n",
            "5    None  None    None   None  \n",
            "6    None  None    None   None  \n",
            "7    None  None    None   None  \n",
            "8    None  None    None   None  \n",
            "9    None  None    None   None  \n",
            "10   None  None    None   None  \n",
            "11   None  None    None   None  \n",
            "12   None  None    None   None  \n",
            "13   None  None    None   None  \n",
            "14   None  None    None   None  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import os\n",
        "\n",
        "# URL of the IMDb page you want to scrape\n",
        "base_url = 'https://mausam.imd.gov.in/'\n",
        "url = base_url\n",
        "\n",
        "# Send a GET request to the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the HTML content of the page\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Find all <img> tags with a specific class\n",
        "image_tags = soup.find_all('img', class_='img-fluid mr-radar scale-x')\n",
        "\n",
        "# Extract the src attribute from each <img> tag\n",
        "image_urls = [img['src'] for img in image_tags]\n",
        "\n",
        "# Download and save each image\n",
        "for i, url in enumerate(image_urls, start=1):\n",
        "    # Construct the absolute URL\n",
        "    absolute_url = base_url + url\n",
        "    # Construct a filename for the image\n",
        "    filename = f'image_{i}.gif'\n",
        "    # Download the image\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(requests.get(absolute_url).content)\n",
        "    print(f'Saved image {i} as {filename}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTpngCWCcPKb",
        "outputId": "2096c12e-ace8-4da5-ccf0-fa4a87809b05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved image 1 as image_1.gif\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import os\n",
        "\n",
        "# URL of the website you want to scrape\n",
        "base_url = 'https://mausam.imd.gov.in/'\n",
        "url = base_url\n",
        "\n",
        "# Send a GET request to the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the HTML content of the page\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Find all <img> tags with a specific class\n",
        "image_tags = soup.find_all('img', class_='img-fluid mr-radar')\n",
        "\n",
        "# Extract the src attribute from each <img> tag\n",
        "image_urls = [img['src'] for img in image_tags]\n",
        "\n",
        "# Download and save each image\n",
        "for i, url in enumerate(image_urls, start=1):\n",
        "    # Construct the absolute URL\n",
        "    absolute_url = base_url + url\n",
        "    # Construct a filename for the image\n",
        "    filename = f'image_{i}.gif'\n",
        "    # Download the image\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(requests.get(absolute_url).content)\n",
        "    print(f'Saved image {i} as {filename}')\n"
      ],
      "metadata": {
        "id": "n8-7j6AWdvJe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f30522b6-1c3e-4f39-ec4b-4547e47047da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved image 1 as image_1.gif\n",
            "Saved image 2 as image_2.gif\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "\n",
        "# The URL of the page to scrape\n",
        "page_url = \"https://www.weatherandradar.in/weather-forecast/IN?date=2024-05-24\"\n",
        "\n",
        "# Send a GET request to fetch the HTML content\n",
        "response = requests.get(page_url)\n",
        "response.raise_for_status()  # Check if the request was successful\n",
        "\n",
        "# Parse the HTML content using BeautifulSoup\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Find the image tag with the specific class and extract the src attribute\n",
        "image_tag = soup.find('img', class_='image')\n",
        "if image_tag:\n",
        "    image_url = image_tag['src']\n",
        "\n",
        "    # Download the image\n",
        "    img_response = requests.get(image_url)\n",
        "    img_response.raise_for_status()  # Check if the request was successful\n",
        "\n",
        "    # Save the image to a file\n",
        "    image_filename = os.path.basename(image_url)\n",
        "    with open(image_filename, 'wb') as img_file:\n",
        "        img_file.write(img_response.content)\n",
        "\n",
        "    print(f\"Image downloaded and saved as {image_filename}\")\n",
        "else:\n",
        "    print(\"Image tag not found.\")\n"
      ],
      "metadata": {
        "id": "RUVFcSZCIWRd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e90aba18-e2d3-4bd7-c315-19348edff21b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image tag not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import requests\n",
        "\n",
        "def download_image(image_url, file_name):\n",
        "    try:\n",
        "        # Send a GET request to fetch the image\n",
        "        response = requests.get(image_url)\n",
        "\n",
        "        # Check if the request was successful\n",
        "        if response.status_code == 200:\n",
        "            # Save the image locally\n",
        "            with open(file_name, 'wb') as file:\n",
        "                file.write(response.content)\n",
        "            print(\"Image successfully downloaded as:\", file_name)\n",
        "        else:\n",
        "            print(\"Failed to retrieve the image. Status code:\", response.status_code)\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred:\", str(e))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # URL of the image\n",
        "    image_url = \"https://api-web.wo-cloud.com/symbolmap/img/v2/maps?map_id=IN&language=en&date=20240524&daytime=day&unit=C&c=d293ZWI6QzhMNFRINmVUbkRoVWFqYg==\"\n",
        "\n",
        "    # File name to save the image\n",
        "    file_name = \"weather_image.png\"\n",
        "\n",
        "    # Call the function to download the image\n",
        "    download_image(image_url, file_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GECrB6RAljB",
        "outputId": "913514dd-7335-4fc8-fa0b-a14e93c05276"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image successfully downloaded as: weather_image.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import os\n",
        "\n",
        "# The HTML content you provided (you might fetch it from a webpage)\n",
        "html_content = \"\"\"\n",
        "<html>\n",
        "<head></head>\n",
        "<body bgcolor=\"#FFFFFF\" text=\"#000000\" data-new-gr-c-s-check-loaded=\"14.1174.0\" data-gr-ext-installed>\n",
        "<table width=\"651\" border=\"0\">\n",
        "<tbody>\n",
        "<tr></tr>\n",
        "<tr></tr>\n",
        "<tr>\n",
        "<td>\n",
        "<table width=\"678\" border=\"0\">\n",
        "<tbody>\n",
        "<tr valign=\"top\">\n",
        "<td class=\"plaintext\" width=\"113\">\n",
        "<a href=\"ahmegfs.png\">Ahmedabad</a><br>\n",
        "<a href=\"banggfs.png\">Bangalore</a><br>\n",
        "<a href=\"bhopgfs.png\">Bhopal</a><br>\n",
        "\n",
        "\n",
        "<!-- More cities here -->\n",
        "</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>\n",
        "</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Base URL for the images\n",
        "base_url = \"http://www.monsoondata.org/wx2/\"\n",
        "\n",
        "# Directory to save the images\n",
        "image_dir = \"meteogram_images\"\n",
        "os.makedirs(image_dir, exist_ok=True)\n",
        "\n",
        "# Parse the HTML content\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# Find all <a> tags within the specified table structure\n",
        "city_data = []\n",
        "for a_tag in soup.find_all('a'):\n",
        "    city_name = a_tag.text\n",
        "    png_link = base_url + a_tag['href']\n",
        "    city_data.append((city_name, png_link))\n",
        "\n",
        "# Fetch and save each image\n",
        "for city in city_data:\n",
        "    city_name = city[0]\n",
        "    png_link = city[1]\n",
        "    response = requests.get(png_link)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        image_path = os.path.join(image_dir, f\"{city_name}.png\")\n",
        "        with open(image_path, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        print(f\"Saved {city_name} meteogram to {image_path}\")\n",
        "    else:\n",
        "        print(f\"Failed to fetch image for {city_name}\")\n",
        "\n",
        "# Print the extracted data\n",
        "for city in city_data:\n",
        "    print(f\"City: {city[0]}, PNG Link: {city[1]}\")\n"
      ],
      "metadata": {
        "id": "_I2Px3CMBV2X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b20e631-4f83-46c1-9aae-32f600eede51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved Ahmedabad meteogram to meteogram_images/Ahmedabad.png\n",
            "Saved Bangalore meteogram to meteogram_images/Bangalore.png\n",
            "Saved Bhopal meteogram to meteogram_images/Bhopal.png\n",
            "City: Ahmedabad, PNG Link: http://www.monsoondata.org/wx2/ahmegfs.png\n",
            "City: Bangalore, PNG Link: http://www.monsoondata.org/wx2/banggfs.png\n",
            "City: Bhopal, PNG Link: http://www.monsoondata.org/wx2/bhopgfs.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install aiohttp beautifulsoup4 nest_asyncio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNmqmcpnX-BJ",
        "outputId": "199466e9-2dbf-49df-8a9b-229a8f52ce1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (3.10.5)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp) (3.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from requests.packages.urllib3.util.retry import Retry\n",
        "\n",
        "# URL and headers\n",
        "url = 'https://www.livecareer.com/'  # Update the URL as needed\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36',\n",
        "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "    'Accept-Encoding': 'gzip, deflate, br',  # Support compressed responses\n",
        "    'Accept-Language': 'en-US,en;q=0.5'\n",
        "}\n",
        "\n",
        "# Create a session object to reuse connections\n",
        "session = requests.Session()\n",
        "\n",
        "# Set up retries with backoff strategy\n",
        "retry = Retry(\n",
        "    total=3,             # Total retry attempts\n",
        "    read=3,              # Retry on read errors\n",
        "    connect=3,           # Retry on connection errors\n",
        "    backoff_factor=0.5,  # Wait between retries (0.5 seconds, doubles with each retry)\n",
        "    status_forcelist=[500, 502, 503, 504]  # Retry on these HTTP status codes\n",
        ")\n",
        "adapter = HTTPAdapter(max_retries=retry)\n",
        "session.mount('http://', adapter)\n",
        "session.mount('https://', adapter)\n",
        "\n",
        "try:\n",
        "    # Send the GET request with a 10-second timeout\n",
        "    response = session.get(url, headers=headers, timeout=10)\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        print(\"Page successfully fetched.\")\n",
        "\n",
        "        # Parse the page and find the image (assuming BeautifulSoup is needed)\n",
        "        from bs4 import BeautifulSoup\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        image_div = soup.find('div', class_='document fontsize fontface vmargins hmargins pagesize skn-upt2 UPT2 LCA')\n",
        "\n",
        "        # If the div contains an image, download and save it\n",
        "        if image_div:\n",
        "            img_tag = image_div.find('img')\n",
        "            if img_tag:\n",
        "                img_url = img_tag['src']  # Extract image URL\n",
        "                img_response = session.get(img_url, timeout=10)\n",
        "\n",
        "                # Save the image to a local file\n",
        "                with open('scraped_image.png', 'wb') as img_file:\n",
        "                    img_file.write(img_response.content)\n",
        "                print(\"Image saved successfully.\")\n",
        "            else:\n",
        "                print(\"Image tag not found in the specified div.\")\n",
        "        else:\n",
        "            print(\"Specified div not found on the page.\")\n",
        "    else:\n",
        "        print(f\"Failed to fetch the page. Status code: {response.status_code}\")\n",
        "\n",
        "except requests.exceptions.Timeout:\n",
        "    print(\"The request timed out. Please try again later.\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-Ms11B_aYXg",
        "outputId": "8128046b-6519-459a-e582-9b8a15232ca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=3, read=2, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.livecareer.com', port=443): Read timed out. (read timeout=10)\")': /\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=3, read=1, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.livecareer.com', port=443): Read timed out. (read timeout=10)\")': /\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=3, read=0, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.livecareer.com', port=443): Read timed out. (read timeout=10)\")': /\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: HTTPSConnectionPool(host='www.livecareer.com', port=443): Max retries exceeded with url: / (Caused by ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2o37XGM6aeBl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}